```{r, echo = FALSE, warning=FALSE, include=FALSE}
# Project 1 R script
# data loading
# link to webpage with data: https://www.lendingclub.com/info/download-data.action
mainData <- read.csv(file = "C:\\Users\\Allen\\Desktop\\Data Science in R\\project1\\LoanStats3a.csv")

# Libraries used
library(gbm)
library(ROCR)
library(e1071)
library(stringr)
library(tree)
library(randomForest)

# data cleaning
mainData$member_id <- NULL
mainData$url <- NULL
mainData$desc <- NULL
mainData$purpose <- NULL
mainData$title <- NULL
mainData$zip_code <- NULL
mainData$addr_state <- NULL
mainData$mths_since_last_record <- NULL
mainData$initial_list_status <- NULL
mainData$out_prncp_inv <- NULL
mainData$total_pymnt <- NULL
mainData$recoveries <- NULL
mainData$collection_recovery_fee <- NULL
mainData$last_credit_pull_d <- NULL
mainData$collections_12_mths_ex_med <- NULL
mainData$mths_since_last_major_derog <- NULL
mainData$policy_code <- NULL
mainData$application_type <- NULL
mainData$annual_inc_joint <- NULL
mainData$dti_joint <- NULL
mainData$verification_status_joint <- NULL
mainData$acc_now_delinq <- NULL
mainData$tot_coll_amt <- NULL
mainData$tot_cur_bal <- NULL
mainData$open_acc_6m <- NULL
mainData[82:145] <- NULL
mainData[35:80] <- NULL
mainData$percent_laon_rec <- mainData$funded_amnt/mainData$loan_amnt

# collecting only the complete cases
mainData <- mainData[complete.cases(mainData),]

# imputing(kind of)
mainData$id <- 1:length(mainData$id)

# removing whitespace
mainData$int_rate <- trimws(str_split_fixed(str_split_fixed(mainData$int_rate, "%", 2)[,1], " ",2)[,2], which = c("both", "left", "right"))

# turning variabl data into number values only
mainData$home_ownership <- as.integer(mainData$home_ownership == "OWN")
mainData$a_grade <- as.integer(mainData$grade == "A")
mainData$b_grade <- as.integer(mainData$grade == "B")
mainData$c_grade <- as.integer(mainData$grade == "C")
mainData$d_grade <- as.integer(mainData$grade == "D")
mainData$e_grade <- as.integer(mainData$grade == "E")
mainData$term <- as.integer(str_split_fixed(mainData$term, " ", 3)[,2])
```
# Project 2 Markdown
###### By: Allen Thornberry

***
# Table of Contents
- [Data Acquisition]
- [Data Description]
- [Background Information]
- [SVM Model Results]
- [Linear Regression Model Results]
- [Decision Tree Model Results]

***
# Data Acquisition
This report is based on the data from [Lending Club](https://www.lendingclub.com/info/download-data.action). In order to download the data, go to the link embedded in the previous sentence and click the button under the header that says "Download Loan Data, 2007-2011." To recreate any of the results in this report, the data set will need to be downloaded, and the data will need to be inserted in the `read.csv()`(found in the .Rmd file provided) lines with the new file path created on the computer. The file path should be written in a form similar to `C:\\user\\file\\data.csv`.

***
# Data Description
This file contains a complete set of loan data for all loans issued through the time period stated(2007 - 2011) through the Lending Club, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. Also the loan data contains complete data for all loans issued through the previous completed calendar quarter(up through October, 2011). Some variables from the original data set have been excluded because they were not important to create the findings. The important variables in the data set include:

* ID - numbered by number of loans from 1 - 42540
* loan_amnt - requested amount of each laon
* funded_amnt - actual given amount for loan
* funded_amnt_inv - amount of the funded amount actually invested
* term - number of payments on loan, in terms of month
* int_rate - interest rate of the loan
* installment - monthly payment amount
* grade - how risky the loan reciever is
* sub_grade - a more specific quality score of loan receiver
* emp_title - name of company to take out the loans
* emp_length - how long the business has been open(0 means less than a year, and 10 means greater than or equal to a year)
* home_ownership - in terms of rent, own, mortgage, and other
* annual_income - self reported annual income of loan reciever
* verification_status - states whether the source was verified or not
* issue_d - date loan was issued
* loan_status - status of the loan
* pymnt_plan - indicates if there has been a payment plan implemented for the loan
* dti - A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.
* delinq_2yrs - the number of times a payment was 30+ days past due in the last two years
* earliest_cr_line - date the borrower's earliest line of credit was opened
* inq_last_6mths - number of inquiries in the last 6 months(exculding car and mortgage inquiries)
* mths_since_last_delinq - months since last delinquent payment
* open_acc - number of open cedit lines in borrower's credit file
* revol_bal - how much has not been taken out on a node
* revol_util - ratio of balance used
* total_acc - total number of credit lines in borrower's credit file
* out_prncp - oustanding principal
* total_pymnt - payment recieved for funded amount
* total_rec_prncp - total principle recieved to date
* total_rec_int - total interest recieved to date
* total_rec_late_fee - total amount earned in late fees
* last_pymnt_d - last payment's date
* last_pymnt_amount - last payment's paid amount
* pub_rec_bankruptcies - number of recorded public bankruptcies

Back to [Table of Contents]

***
#Background Information

In this report three machine learning models will be used to predict whether a person owns a home or not, based on information about the individual when the loan was taken out. This process will be done with the following models:

- Support Vector Machine(SVM)
- Logistical Linear Regression
- Decision Tree Modeling

Each model will develop its own algorithm to calculate whether a person is a home owner or not by training itself on smaller sections of the main data set and testing itself on a validation/test set of data. The data will be broken up into numerous groups, and the data will be distributed randomly between those groups. This will be done using the following code.


```{r, echo = TRUE}
mainData$group <- sample(1:12, nrow(mainData), replace = TRUE)
```


The accuracy of each model will be calculated using a AUC-ROC value and curve. AUC stands for the Area Under the Curve, and ROC stands for Reciever Operating Characteristic. The ROC curve measures the probability of of an event happening, while the AUC is measuring the area under the curve to show the precentage of the probability.

The modeling variables that will be used in order to create an effective madel are as follow: annual income, dti or debt ration, number of public recorded bankruptcies, and whether the person has a b or d lending grade.

*Values deteremined by using a function that repeatedly tested variables to find the best options for a linear regression(which will be explained later)*

```{r, echo=FALSE}
model_var <- c("annual_inc", "dti", "pub_rec_bankruptcies", "b_grade", "d_grade")

# "e_grade", "c_grade", "total_acc", "a_grade",  "term", "loan_amnt",  "open_acc",
```

If the accuracy in predicting home ownership is not high that means there is not a strong correlation between the given data and whether someone owns a home or not.

*All the models in this section are used to predict the likelihood of home ownership*

Back to [Table of Contents]

***
# SVM Model Results

A Support Vector Machine, SVM for short, creates seperates data into classifications. This classification is a hyperplane that most accurately puts the largest number of sinilar classifications on the same side. I chose this model because home ownership can be divided into two catagories, owned or it is not owned.

The results for this model are shown by the following graph(the line represents the accuracy of the model):

```{r, echo=FALSE}
#mainData <- mainData[1:10000,]
for (group in 1:12){
  train <- mainData[mainData$group != group, c(model_var, "home_ownership")]
  test <- mainData[mainData$group == group, model_var]
  
  model <- svm(home_ownership ~ ., train, cost = .5, tolerance = .01)
  
  # predictions placed at end of the data sets
  mainData$prediction[mainData$group == group] <- predict(model, test, type = 'response')
  train$prediction <- predict(model, train)
}

# auc rating stuff for end result and all cross validation
pr <- ROCR::prediction(mainData$prediction, mainData$home_ownership)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
```

The following value is the ROC-AUC:

```{r, echo=FALSE}
print(auc)
```

Now we can view the probability curve for the models training data(we want the graphs to look the similar so we know that the model is not trained too tightly to the training data set).

```{r, echo=FALSE}
# auc rating stuff for training set
pr <- ROCR::prediction(train$prediction, train$home_ownership)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
```

Here is the ROC-AUC for the training set(again we want this value to be as close as possible to the AUC from above).

```{r, echo=FALSE}
print(auc)
```

Back to [Table of Contents]

***
# Linear Regression Model Results

Linear regression is the simplest and oldest modeling method that exists. Before computers existed humans used linear regression to determine a line of best fit on a scatter plot. This line of best fit would allow for the estimation or prediction of where a new point of data would be placed. This method is very easy for the human eye to get accustomed too, invlolving the development of an eyeballing skill. In order to determine how accurate a linear regression is, an r squared test is used. This test calculates the square of the difference of two points on either side best fit line. Luckily ROC-AUC, the accuracy test for this project, can still show the accuracy for a computer generated lnear regression model.

The following graph is a visual representation of the accuracy of the results(the graphs will follow the same pattern as above, we want the graphs and AUC values to be as similar as possible).

```{r, echo=FALSE, warning=FALSE}
for (group in 1:12){
  train <- mainData[mainData$group != group, c(model_var, "home_ownership")]
  test <- mainData[mainData$group == group, model_var]
  
  model <- glm(home_ownership ~ ., train, family=binomial(link='logit'))
  
  # predictions placed at end of the data sets
  mainData$prediction[mainData$group == group] <- predict(model, test, type = 'response')
  train$prediction <- predict(model, train)
}

# auc rating stuff for end result and all cross validation
pr <- ROCR::prediction(mainData$prediction, mainData$home_ownership)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
```

The training resulted in an AUC of:

```{r, echo=FALSE}
print(auc)
```

This is what the accuracy of the training set looked like for this model:

```{r, echo=FALSE}
# auc rating stuff for training set
pr <- ROCR::prediction(train$prediction, train$home_ownership)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
```

The training AUC was:

```{r, echo=FALSE}
print(auc)
```

Back to [Table of Contents]

***
# Decision Tree Model Results

A decision tree is a model that develops its predictions by creating a tree with different possibilities, those possibilities being the variables chosen for modeling. As the model trains itself, it asigns weights as probability to each variable in the tree. These weights help the tree determine which path to take with new data given to the set. This model was chosen because it makes decisions similar to how humans make decisions(If this happens, then I do this). A random forest model was chosen specifically for this data set because it uses a process called bagging to reduce how overfit the model gets(an over fit model means that the model did very well on its test set, but can't appropriately predict the results in a testing set).

```{r, echo=FALSE, warning=FALSE}
for (group in 1:12){
  train <- mainData[mainData$group != group, c(model_var, "home_ownership")]
  test <- mainData[mainData$group == group, model_var]
  
  model <- randomForest(home_ownership ~ ., data=train, ntree = 1000)
  
  # predictions placed at end of the data sets
  mainData$prediction[mainData$group == group] <- predict(model, test, type = 'response')
  train$prediction <- predict(model, train)
}
```

Like the previous models, this model also will use two graphs to display how well the model performed. This models resulting accuracy graph is as follows:

```{r, echo=FALSE}
# auc rating stuff for end result and all cross validation
pr <- ROCR::prediction(mainData$prediction, mainData$home_ownership)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
```

The resulting AUC was:

```{r, echo=FALSE}
print(auc)
```

The training sets accuracy results are as follows:

```{r, echo=FALSE}
# auc rating stuff for training set
pr <- ROCR::prediction(train$prediction, train$home_ownership)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
```

The training sets AUC was:

```{r, echo=FALSE}
print(auc)
```

Back to [Table of Contents]

***
#### Index
- [Project 2 Markdown]
- [Data Acquisition]
- [Data Description]
- [Background Information]
- [SVM Model Results]
- [Linear Regression MOdel Results]
- [Decision Tree Model Results]